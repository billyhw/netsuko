---
title: "Optimization"
author: "Sakura Innovation"
date: created - 2020-11-26, last updated - `r Sys.time()` (GMT+8)
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
```

```{r, fig.width = 3.5, fig.height = 3.5}
set.seed(888)
x = rnorm(100)
y = x + rnorm(100, sd = 0.5)
plot(x, y)
title("I am data~~~")
```

```{r}
get_error = function(x, y, alpha, beta) y - (alpha + beta*x)
cost_func = function(r) mean(r^2)
grad_alpha = function(r) -2*mean(r)
grad_beta = function(r, x) -2*mean(r*x)
```

```{r}
step_size_vec = c(0.01, 0.1, 0.91)
cost = matrix(NA, 100, 3)
colnames(cost) = step_size_vec
alpha_vec = beta_vec = rep(NA, 3)
for (j in 1:3) {
  set.seed(888)
  alpha = rnorm(1, sd = 0.1)
  beta = rnorm(1, sd = 0.1)
  r = get_error(x, y, alpha, beta)
  cost[1, j] = cost_func(r)
  for (i in 2:100) {
    alpha = alpha - step_size_vec[j] * grad_alpha(r)
    beta = beta - step_size_vec[j] * grad_beta(r, x)
    r = get_error(x, y, alpha, beta)
    cost[i, j] = cost_func(r)
  }
  alpha_vec[j] = alpha
  beta_vec[j] = beta
}
```

```{r, fig.width = 4, fig.height = 2.5}
as.data.frame(cost) %>%
  mutate(iter = 1:nrow(cost)) %>%
  pivot_longer(!iter, names_to = "step_size", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = step_size)) +
  geom_line(lwd = 1.5) +
  scale_y_log10() +
  xlab("iteration")
```


```{r}
result = rbind(alpha_vec, beta_vec)
result = cbind(result, lm(y~x)$coef)
rownames(result) = c("Intercept", "Slope")
colnames(result) = c(paste("step size = ", step_size_vec, sep = ""), "exact")

as.data.frame(result) %>%
  knitr::kable(format = "html", digits = 3) %>%
  kableExtra::kable_styling()
```

# Logistic Regression

```{r}
logistic_activation = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r, fig.width = 4, fig.height = 3}
data.frame(x = seq(-15, 15, length = 100)) %>%
  mutate(`alpha=0` = logistic_activation(0, 1, x), `alpha=-3` = logistic_activation(-5, 1, x), `alpha=3` = logistic_activation(5, 1, x)) %>%
  pivot_longer(!x, names_to = "alpha", values_to = "P(y|x)") %>%
  ggplot(aes(x = x, y = `P(y|x)`, col = alpha)) +
  geom_line() + 
  ggtitle("beta = 1")
```

```{r, fig.width = 4, fig.height = 3}
data.frame(x = seq(-15, 15, length = 100)) %>%
  mutate(`beta=0` = logistic_activation(0, 0, x), `beta=-3` = logistic_activation(0, -1, x), `beta=3` = logistic_activation(0, 1, x)) %>%
  pivot_longer(!x, names_to = "beta", values_to = "P(y|x)") %>%
  ggplot(aes(x = x, y = `P(y|x)`, col = beta)) +
  geom_line() + 
  ggtitle("alpha = 0")
```

```{r, fig.width = 5, fig.height = 3}
data.frame(x = seq(-15, 15, length = 100)) %>%
  mutate(logistic_activation(0, 1, x)) %>%
  pivot_longer(!x, names_to = "beta", values_to = "P(y|x)") %>%
  ggplot(aes(x = x, y = `P(y|x)`, col = beta)) +
  geom_line(lwd = 3) + 
  ggtitle("alpha = 0")
```

```{r}
set.seed(888)
x = rnorm(100)
y = rbinom(100, 1, prob = logistic_activation(alpha = 0, beta = 1, x))
```

```{r}
get_p = function(x, alpha, beta) logistic_activation(alpha, beta, x)
get_error = function(y, p) y - p
cost_func = function(p, y) -sum(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(r) -sum(r)
grad_beta = function(r, x) -sum(r*x)
```

```{r}
step_size_vec = c(0.001, 0.01, 0.1)
cost = matrix(NA, 100, 3)
colnames(cost) = step_size_vec
alpha_vec = beta_vec = rep(NA, 3)
for (j in 1:3) {
  set.seed(888)
  alpha = rnorm(1, sd = 0.1)
  beta = rnorm(1, sd = 0.1)
  p = get_p(x, alpha, beta)
  r = get_error(y, p)
  cost[1, j] = cost_func(p, y)
  for (i in 2:100) {
    alpha = alpha - step_size_vec[j] * grad_alpha(r)
    beta = beta - step_size_vec[j] * grad_beta(r, x)
    p = get_p(x, alpha, beta)
    r = get_error(y, p)
    cost[i, j] = cost_func(p, y)
  }
  alpha_vec[j] = alpha
  beta_vec[j] = beta
}
```

```{r, fig.width = 4, fig.height = 2.5}
as.data.frame(cost) %>%
  mutate(iter = 1:nrow(cost)) %>%
  pivot_longer(!iter, names_to = "step_size", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = step_size)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
result = rbind(alpha_vec, beta_vec)
result = cbind(result, glm(y~x, family = "binomial")$coef)
rownames(result) = c("Intercept", "Slope")
colnames(result) = c(paste("step size = ", step_size_vec, sep = ""), "exact")

as.data.frame(result) %>%
  knitr::kable(format = "html", digits = 3) %>%
  kableExtra::kable_styling()
```

# Neural Networks

```{r}
logistic_activation = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
output_func = function(alpha, beta, w0, w1, x) {
  z = logistic_activation(w0, w1, x)
  logistic_activation(alpha, beta, z)
}
```

```{r}
output_func_2 = function(alpha, beta1, beta2, w10, w11, w20, w21, x) {
  z_1 = logistic_activation(w10, w11, x)
  z_2 = logistic_activation(w20, w21, x)
  1/(1 + exp(-(alpha + beta1*z_1 + beta2*z_2)))
}
```

```{r, fig.width = 4, fig.height = 3}
data.frame(x = seq(-15, 15, length = 100)) %>%
  mutate(`alpha=0` = output_func(-0.5, 1, 0, -1, x), `alpha=-3` = output_func(-0.1, 1, 0, 1, x), `alpha=3` = output_func(1, -1, 0, -1, x)) %>%
  pivot_longer(!x, names_to = "alpha", values_to = "P(y|x)") %>%
  ggplot(aes(x = x, y = `P(y|x)`, col = alpha)) +
  geom_line() + 
  ggtitle("1 hidden unit")
```

```{r, fig.width = 4, fig.height = 3}
data.frame(x = seq(-15, 15, length = 100)) %>%
  mutate(`alpha=0` = output_func_2(-0.5, 1, 0.5, 0, -1, -3, 1, x), `alpha=-3` = output_func_2(-0.1, 1, -3, 0, -1, 1, -2, x), `alpha=3` = output_func_2(1, -3, -3, 5, 2, -3, 1,x)) %>%
  pivot_longer(!x, names_to = "alpha", values_to = "P(y|x)") %>%
  ggplot(aes(x = x, y = `P(y|x)`, col = alpha)) +
  geom_line() + 
  ggtitle("2 hidden units")
```

```{r, fig.width = 4, fig.height = 3}
data.frame(x = seq(-15, 15, length = 100)) %>%
  mutate(`beta=0` = logistic_activation(0, 0, x), `beta=-3` = logistic_activation(0, -1, x), `beta=3` = logistic_activation(0, 1, x)) %>%
  pivot_longer(!x, names_to = "beta", values_to = "P(y|x)") %>%
  ggplot(aes(x = x, y = `P(y|x)`, col = beta)) +
  geom_line() + 
  ggtitle("alpha = 0")
```


```{r}
logistic_activation = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(888)
x = rnorm(100)
y = rbinom(100, 1, prob = logistic_activation(alpha = 0, beta = 1, x))
```

```{r}
get_t = function(z_1, z_2, alpha, beta_1, beta_2) alpha + beta_1*z_1 + beta_2*z_2
get_s = function(x, w_0, w_1) w_0 + w_1*x
activation = function(s) 1/(1 + exp(-s))
get_error_2 = function(y, p) y - p
get_error_1 = function(delta_2, grad_s, beta) grad_s*beta*delta_2
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -sum(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(delta_2) -sum(delta_2)
grad_beta = function(delta_2, z) -sum(delta_2*z)
grad_w0 = function(delta_1) -sum(delta_1)
grad_w1 = function(delta_1, x) -sum(delta_1*x)
```

```{r}
forward_backward_pass = function(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21) {
  # compute the linear predictors and hidden units
  s_1 = get_s(x, w_10, w_11)
  z_1 = activation(s_1)
  s_2 = get_s(x, w_20, w_21)
  z_2 = activation(s_2)
  # compute the output units
  t_1 = get_t(z_1, z_2, alpha, beta_1, beta_2)
  p = activation(t_1)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s1 = grad_logistic(s_1)
  grad_s2 = grad_logistic(s_2)
  delta_11 = get_error_1(delta_2, grad_s1, beta_1)
  delta_12 = get_error_1(delta_2, grad_s2, beta_2)
  return(ls = list(p=p, delta_2=delta_2, delta_11=delta_11, delta_12=delta_12, 
                   z_1=z_1, z_2=z_2))
}
```

```{r}
step_size_vec = c(0.01, 0.025, 0.04)
cost = matrix(NA, 1000, 3)
colnames(cost) = step_size_vec
alpha_vec = beta_vec = rep(NA, 3)
for (j in 1:3) {
  set.seed(888)
  # parameter initialization
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  # get initial cost
  fb = forward_backward_pass(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  cost[1, j] = cost_func(fb$p, y)
  # gradient descent
  for (i in 2:1000) {
    alpha = alpha - step_size_vec[j] * grad_alpha(fb$delta_2)
    beta_1 = beta_1 - step_size_vec[j] * grad_beta(fb$delta_2, fb$z_1)
    beta_2 = beta_2 - step_size_vec[j] * grad_beta(fb$delta_2, fb$z_2)
    w_10 = w_10 - step_size_vec[j] * grad_w0(fb$delta_11)
    w_20 = w_20 - step_size_vec[j] * grad_w0(fb$delta_12)
    w_11 = w_11 - step_size_vec[j] * grad_w1(fb$delta_11, x)
    w_21 = w_21 - step_size_vec[j] * grad_w1(fb$delta_12, x)
    fb = forward_backward_pass(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    cost[i, j] = cost_func(fb$p, y)
  }
}
```

```{r, fig.width = 4, fig.height = 2.5}
as.data.frame(cost) %>%
  mutate(iter = 1:nrow(cost)) %>%
  pivot_longer(!iter, names_to = "step_size", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = step_size)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```


```{r}
step_size_vec = c(0.01, 0.025, 0.08)
cost = matrix(NA, 1000, 3)
colnames(cost) = step_size_vec
alpha_vec = beta_vec = rep(NA, 3)
for (j in 1:3) {
  set.seed(888)
  # parameter initialization
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  # get initial cost
  fb = forward_backward_pass(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  cost[1, j] = cost_func(fb$p, y)
  # gradient descent
  for (i in 2:1000) {
    alpha = alpha - step_size_vec[j] * grad_alpha(fb$delta_2)
    beta_1 = beta_1 - step_size_vec[j] * grad_beta(fb$delta_2, fb$z_1)
    beta_2 = beta_2 - step_size_vec[j] * grad_beta(fb$delta_2, fb$z_2)
    w_10 = w_10 - step_size_vec[j] * grad_w0(fb$delta_11)
    w_20 = w_20 - step_size_vec[j] * grad_w0(fb$delta_12)
    w_11 = w_11 - step_size_vec[j] * grad_w1(fb$delta_11, x)
    w_21 = w_21 - step_size_vec[j] * grad_w1(fb$delta_12, x)
    fb = forward_backward_pass(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    cost[i, j] = cost_func(fb$p, y)
  }
}
```

```{r, fig.width = 4, fig.height = 2.5}
as.data.frame(cost) %>%
  mutate(iter = 1:nrow(cost)) %>%
  pivot_longer(!iter, names_to = "step_size", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = step_size)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
seed_vec = c(8, 888)
cost = matrix(NA, 10000, 2)
colnames(cost) = seed_vec
for (j in 1:2) {
  set.seed(seed_vec[j])
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  fb = forward_backward_pass(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  cost[1, j] = cost_func(fb$p, y)
  for (i in 2:10000) {
    alpha = alpha - 0.04 * grad_alpha(fb$delta_2)
    beta_1 = beta_1 - 0.04 * grad_beta(fb$delta_2, fb$z_1)
    beta_2 = beta_2 - 0.04 * grad_beta(fb$delta_2, fb$z_2)
    w_10 = w_10 - 0.04 * grad_w0(fb$delta_11)
    w_20 = w_20 - 0.04 * grad_w0(fb$delta_12)
    w_11 = w_11 - 0.04 * grad_w1(fb$delta_11, x)
    w_21 = w_21 - 0.04 * grad_w1(fb$delta_12, x)
    fb = forward_backward_pass(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    cost[i, j] = cost_func(fb$p, y)
  }
}
```

```{r, fig.width = 4, fig.height = 2.5}
as.data.frame(cost) %>%
  mutate(iter = 1:nrow(cost)) %>%
  pivot_longer(!iter, names_to = "seed", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = seed)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r, fig.width = 4, fig.height = 3.5}
x_min_vec = rep(NA, 100)
x_min_vec[1] = -2
for (i in 2:100) x_min_vec[i] = x_min_vec[i-1] - 0.1*(2*x_min_vec[i-1])

x = seq(-2, 2, length = 100)
plot(x, x^2, type = "l")
points(x_min_vec, x_min_vec^2, pch = 19)
```

```{r, fig.width = 4, fig.height = 3.5}
x_min_vec_1 = x_min_vec_2 = rep(NA, 100)

x_min_vec_1[1] = -4
for (i in 2:1000) x_min_vec_1[i] = x_min_vec_1[i-1] - 0.2*(0.1 + cos(x_min_vec_1[i-1]))

x_min_vec_2[1] = 2
for (i in 2:1000) x_min_vec_2[i] = x_min_vec_2[i-1] - 0.2*(0.1 + cos(x_min_vec_2[i-1]))

x = seq(-5, 7, length = 100)
plot(x, 0.1*x + sin(x), type = "l")
points(x_min_vec_1, 0.1*x_min_vec_1 + sin(x_min_vec_1), pch = 19)
points(x_min_vec_2, 0.1*x_min_vec_2 + sin(x_min_vec_2), pch = 19, col = 2)
```

# Regularization

```{r}
set.seed(8)
x_train = rnorm(100)
y_train = rbinom(100, 1, prob = logistic_activation(alpha = 0, beta = 1, x_train))
x_test = rnorm(1000)
y_test = rbinom(1000, 1, prob = logistic_activation(alpha = 0, beta = 1, x_test))
```

```{r}
get_p = function(x, alpha, beta) logistic_activation(alpha, beta, x)
get_error = function(y, p) y - p
cost_func = function(p, y) -mean(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(r) -mean(r)
grad_beta = function(r, x) -mean(r*x)
```

```{r}
logistic_regression = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.005) {
  cost_train = cost_test = rep(NA, iter)
  alpha = rnorm(1, sd = 0.1)
  beta = rnorm(1, sd = 0.1)
  p_train = get_p(x_train, alpha, beta)
  r_train = get_error(y_train, p_train)
  cost_train[1] = cost_func(p_train, y_train)
  p_test = get_p(x_test, alpha, beta)
  r_test = get_error(y_test, p_test)
  cost_test[1] = cost_func(p_test, y_test)
  for (i in 2:iter) {
    alpha = alpha - step_size * grad_alpha(r_train)
    beta = beta - step_size * grad_beta(r_train, x_train)
    p_train = get_p(x_train, alpha, beta)
    r_train = get_error(y_train, p_train)
    cost_train[i] = cost_func(p_train, y_train)
    p_test = get_p(x_test, alpha, beta)
    r_test = get_error(y_test, p_test)
    cost_test[i] = cost_func(p_test, y_test)
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test))
}
```

```{r}
set.seed(8)
fit_logistic = logistic_regression(x_train, y_train, x_test, y_test) 
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit_logistic$cost_train, test_cost = fit_logistic$cost_test) %>%
  mutate(iter = 1:length(fit_logistic$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
get_t = function(z_1, z_2, alpha, beta_1, beta_2) alpha + beta_1*z_1 + beta_2*z_2
get_s = function(x, w_0, w_1) w_0 + w_1*x
activation = function(s) 1/(1 + exp(-s))
get_error_2 = function(y, p) y - p
get_error_1 = function(delta_2, grad_s, beta) grad_s*beta*delta_2
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(delta_2) -mean(delta_2)
grad_beta = function(delta_2, z) -mean(delta_2*z)
grad_w0 = function(delta_1) -mean(delta_1)
grad_w1 = function(delta_1, x) -mean(delta_1*x)
```

```{r}
forward_backward_pass = function(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21) {
  # compute the linear predictors and hidden units
  s_1 = get_s(x, w_10, w_11)
  z_1 = activation(s_1)
  s_2 = get_s(x, w_20, w_21)
  z_2 = activation(s_2)
  # compute the output units
  t_1 = get_t(z_1, z_2, alpha, beta_1, beta_2)
  p = activation(t_1)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s1 = grad_logistic(s_1)
  grad_s2 = grad_logistic(s_2)
  delta_11 = get_error_1(delta_2, grad_s1, beta_1)
  delta_12 = get_error_1(delta_2, grad_s2, beta_2)
  return(ls = list(p=p, delta_2=delta_2, delta_11=delta_11, delta_12=delta_12, 
                   z_1=z_1, z_2=z_2))
}
```

```{r}
neural_network_2_units = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  train_p = matrix(NA, length(x_train), iter)
  test_p = matrix(NA, length(x_test), iter)
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  train_p[,1] = fb_train$p
  test_p[,1] = fb_test$p
  for (i in 2:iter) {
    alpha = alpha - step_size * grad_alpha(fb_train$delta_2)
    beta_1 = beta_1 - step_size * grad_beta(fb_train$delta_2, fb_train$z_1)
    beta_2 = beta_2 - step_size * grad_beta(fb_train$delta_2, fb_train$z_2)
    w_10 = w_10 - step_size * grad_w0(fb_train$delta_11)
    w_20 = w_20 - step_size * grad_w0(fb_train$delta_12)
    w_11 = w_11 - step_size * grad_w1(fb_train$delta_11, x_train)
    w_21 = w_21 - step_size * grad_w1(fb_train$delta_12, x_train)
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    train_p[,i] = fb_train$p
    test_p[,i] = fb_test$p
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_10 = w_10, w_11 = w_11, w_20 = w_20, w_21 = w_21,
                   train_p = train_p, test_p = test_p))
}
```

```{r}
set.seed(8)
fit = neural_network_2_units(x_train, y_train, x_test, y_test, step_size = 0.8) 
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```
```{r}
get_t_3_units = function(z_1, z_2, z_3, alpha, beta_1, beta_2, beta_3) alpha + beta_1*z_1 + beta_2*z_2 + beta_3*z_3
```

```{r}
forward_backward_pass_3_units = function(x, y, alpha, beta_1, beta_2, beta_3, w_10, w_11, w_20, w_21, w_30, w_31) {
  # compute the linear predictors and hidden units
  s_1 = get_s(x, w_10, w_11)
  z_1 = activation(s_1)
  s_2 = get_s(x, w_20, w_21)
  z_2 = activation(s_2)
  s_3 = get_s(x, w_30, w_31)
  z_3 = activation(s_3)
  # compute the output units
  t_1 = get_t_3_units(z_1, z_2, z_3, alpha, beta_1, beta_2, beta_3)
  p = activation(t_1)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s1 = grad_logistic(s_1)
  grad_s2 = grad_logistic(s_2)
  grad_s3 = grad_logistic(s_3)
  delta_11 = get_error_1(delta_2, grad_s1, beta_1)
  delta_12 = get_error_1(delta_2, grad_s2, beta_2)
  delta_13 = get_error_1(delta_2, grad_s3, beta_3)
  return(ls = list(p=p, delta_2=delta_2, delta_11=delta_11, delta_12=delta_12, delta_13 = delta_13,
                   z_1=z_1, z_2=z_2, z_3=z_3))
}
```

```{r}
neural_network_3_units = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = auc_train = auc_test = rep(NA, iter)
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  beta_3 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_30 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  w_31 = rnorm(1, sd = 0.1)
  fb_train = forward_backward_pass_3_units(x_train, y_train, alpha, beta_1, beta_2, beta_3, w_10, w_11, w_20, w_21, w_30, w_31)
  fb_test = forward_backward_pass_3_units(x_test, y_test, alpha, beta_1, beta_2, beta_3, w_10, w_11, w_20, w_21, w_30, w_31)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  for (i in 2:iter) {
    alpha = alpha - step_size * grad_alpha(fb_train$delta_2)
    beta_1 = beta_1 - step_size * grad_beta(fb_train$delta_2, fb_train$z_1)
    beta_2 = beta_2 - step_size * grad_beta(fb_train$delta_2, fb_train$z_2)
    beta_3 = beta_3 - step_size * grad_beta(fb_train$delta_2, fb_train$z_3)
    w_10 = w_10 - step_size * grad_w0(fb_train$delta_11)
    w_20 = w_20 - step_size * grad_w0(fb_train$delta_12)
    w_30 = w_30 - step_size * grad_w0(fb_train$delta_13)
    w_11 = w_11 - step_size * grad_w1(fb_train$delta_11, x_train)
    w_21 = w_21 - step_size * grad_w1(fb_train$delta_12, x_train)
    w_31 = w_31 - step_size * grad_w1(fb_train$delta_13, x_train)
    fb_train = forward_backward_pass_3_units(x_train, y_train, alpha, beta_1, beta_2, beta_3, w_10, w_11, w_20, w_21, w_30, w_31)
    fb_test = forward_backward_pass_3_units(x_test, y_test, alpha, beta_1, beta_2, beta_3, w_10, w_11, w_20, w_21, w_30, w_31)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test, auc_train = auc_train, auc_test = auc_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_10 = w_10, w_11 = w_11, w_20 = w_20, w_21 = w_21))
}
```

```{r}
set.seed(8)
fit_3_units = neural_network_3_units(x_train, y_train, x_test, y_test, step_size = 0.8) 
max(diff(fit_3_units$cost_train))
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(training_cost_2_hidden_units = fit$cost_train, test_cost_2_hidden_units = fit$cost_test,
           training_cost_3_hidden_units = fit_3_units$cost_train, test_cost_3_hidden_units = fit_3_units$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(training_cost_2_hidden_units = fit$cost_train, test_cost_2_hidden_units = fit$cost_test,
           training_cost_logistic_regression = fit_logistic$cost_train, test_cost_logistic_regression = fit_logistic$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
neural_network_2_units_weight_decay = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5, lambda = 0.0001) {
  cost_train = cost_test = rep(NA, iter)
  train_p = matrix(NA, length(x_train), iter)
  test_p = matrix(NA, length(x_test), iter)
  x_train = scale(x_train)
  x_test = scale(x_test, attr(x_train, "scaled:center"), attr(x_train, "scaled:scale"))
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  penalty = lambda/2 * (alpha^2 + beta_1^2 + beta_2^2 + w_10^2 + w_20^2 + w_11^2 + w_21^2)
  cost_train[1] = cost_func(fb_train$p, y_train) + penalty
  cost_test[1] = cost_func(fb_test$p, y_test)
  train_p[,1] = fb_train$p
  test_p[,1] = fb_test$p
  for (i in 2:iter) {
    alpha = alpha - step_size * (grad_alpha(fb_train$delta_2) + lambda * alpha)
    beta_1 = beta_1 - step_size * (grad_beta(fb_train$delta_2, fb_train$z_1) + lambda * beta_1)
    beta_2 = beta_2 - step_size * (grad_beta(fb_train$delta_2, fb_train$z_2) + lambda * beta_2)
    w_10 = w_10 - step_size * (grad_w0(fb_train$delta_11) + lambda * w_10)
    w_20 = w_20 - step_size * (grad_w0(fb_train$delta_12) + lambda * w_20)
    w_11 = w_11 - step_size * (grad_w1(fb_train$delta_11, x_train) + lambda * w_11)
    w_21 = w_21 - step_size * (grad_w1(fb_train$delta_12, x_train) + lambda * w_21)
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    penalty = lambda/2 * (alpha^2 + beta_1^2 + beta_2^2 + w_10^2 + w_20^2 + w_11^2 + w_21^2)
    cost_train[i] = cost_func(fb_train$p, y_train) + penalty
    cost_test[i] = cost_func(fb_test$p, y_test)
    train_p[,i] = fb_train$p
    test_p[,i] = fb_test$p
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_10 = w_10, w_11 = w_11, w_20 = w_20, w_21 = w_21,
                   train_p = train_p, test_p = test_p))
}
```

```{r}
set.seed(8)
fit_decay_1 = neural_network_2_units_weight_decay(x_train[1:70], y_train[1:70], x_train[71:100], y_train[71:100], lambda = 0.1, step_size = 0.8, iter = 10000)
fit_decay_01 = neural_network_2_units_weight_decay(x_train[1:70], y_train[1:70], x_train[71:100], y_train[71:100], lambda = 0.01, step_size = 0.8, iter = 10000) 
fit_decay_001 = neural_network_2_units_weight_decay(x_train[1:70], y_train[1:70], x_train[71:100], y_train[71:100], lambda = 0.001, step_size = 0.8, iter = 10000) 
fit_decay_0001 = neural_network_2_units_weight_decay(x_train[1:70], y_train[1:70], x_train[71:100], y_train[71:100], lambda = 0.0001, step_size = 0.8, iter = 10000) 
max(diff(fit_decay_1$cost_train))
max(diff(fit_decay_01$cost_train))
max(diff(fit_decay_001$cost_train))
max(diff(fit_decay_0001$cost_train))
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(test_cost_lambda_0.1 = fit_decay_1$cost_train,
           test_cost_lambda_0.01 = fit_decay_01$cost_train,
           test_cost_lambda_0.001 = fit_decay_001$cost_train,
           test_cost_lambda_0.0001 = fit_decay_0001$cost_train) %>%
  mutate(iter = 1:length(fit_decay_1$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(test_cost_lambda_0.1 = fit_decay_1$cost_test,
           test_cost_lambda_0.01 = fit_decay_01$cost_test,
           test_cost_lambda_0.001 = fit_decay_001$cost_test,
           test_cost_lambda_0.0001 = fit_decay_0001$cost_test) %>%
  mutate(iter = 1:length(fit_decay_1$cost_test)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
set.seed(8)
fit_decay = neural_network_2_units_weight_decay(x_train, y_train, x_test, y_test, lambda = 0.001, step_size = 0.8) 
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(test_cost_2_hidden_units = fit$cost_test,
           test_cost_lambda_0.001 = fit_decay$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
set.seed(8)
fit_validate = neural_network_2_units(x_train[1:70], y_train[1:70], x_train[71:100], y_train[71:100], step_size = 0.5) 
set.seed(8)
fit_early_stop = neural_network_2_units(x_train, y_train, x_test, y_test, step_size = 0.5, iter = which.min(fit_validate$cost_test)) 
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(training_cost = fit_validate$cost_train, validation_cost = fit_validate$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r, fig.width = 6, fig.height = 2.5}
no_early_stop = data.frame(test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost")
early_stop = data.frame(test_cost_early_stop = fit_early_stop$cost_test) %>%
  mutate(iter = 1:length(fit_early_stop$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost")
rbind(no_early_stop, early_stop) %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```


```{r}
neural_network_model_averaging = function(x_train, y_train, x_test, y_test, num_model = 5, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  models = vector("list", num_model)
  train_p = matrix(0, length(x_train), iter)
  test_p = matrix(0, length(x_test), iter)
  for (i in 1:num_model) models[[i]] = neural_network_2_units(x_train, y_train, x_test, y_test)
  for (i in 1:num_model) train_p = train_p + models[[i]]$train_p
  for (i in 1:num_model) test_p = test_p + models[[i]]$test_p
  train_p = train_p/num_model
  test_p = test_p/num_model
  for (i in 1:iter) {
    cost_train[i] = cost_func(train_p[,i], y_train)
    cost_test[i] = cost_func(test_p[,i], y_test)
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test, train_p = train_p, test_p = test_p))
}
```

```{r}
fit_average = neural_network_model_averaging(x_train, y_train, x_test, y_test, num_model = 5, iter = 10000, step_size = 0.8)
```

```{r, fig.width = 6, fig.height = 2.5}
data.frame(test_cost_2_hidden_units = fit$cost_test,
           test_cost_averaged = fit_average$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# Tanh activation

```{r}
get_t = function(z_1, z_2, alpha, beta_1, beta_2) alpha + beta_1*z_1 + beta_2*z_2
get_s = function(x, w_0, w_1) w_0 + w_1*x
tanh_activation = function(s) tanh(s)
logistic_activation = function(s) 1/(1 + exp(-s))
get_error_2 = function(y, p) y - p
get_error_1 = function(delta_2, grad_s, beta) grad_s*beta*delta_2
grad_hidden_activation = function(s) 1-tanh_activation(s)^2
cost_func = function(p, y) -mean(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(delta_2) -mean(delta_2)
grad_beta = function(delta_2, z) -mean(delta_2*z)
grad_w0 = function(delta_1) -mean(delta_1)
grad_w1 = function(delta_1, x) -mean(delta_1*x)
```

```{r}
forward_backward_pass = function(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21) {
  # compute the linear predictors and hidden units
  s_1 = get_s(x, w_10, w_11)
  z_1 = tanh_activation(s_1)
  s_2 = get_s(x, w_20, w_21)
  z_2 = tanh_activation(s_2)
  # compute the output units
  t_1 = get_t(z_1, z_2, alpha, beta_1, beta_2)
  p = logistic_activation(t_1)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s1 = grad_hidden_activation(s_1)
  grad_s2 = grad_hidden_activation(s_2)
  delta_11 = get_error_1(delta_2, grad_s1, beta_1)
  delta_12 = get_error_1(delta_2, grad_s2, beta_2)
  return(ls = list(p=p, delta_2=delta_2, delta_11=delta_11, delta_12=delta_12, 
                   z_1=z_1, z_2=z_2))
}
```

```{r}
neural_network_2_units = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  train_p = matrix(NA, length(x_train), iter)
  test_p = matrix(NA, length(x_test), iter)
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  train_p[,1] = fb_train$p
  test_p[,1] = fb_test$p
  for (i in 2:iter) {
    alpha = alpha - step_size * grad_alpha(fb_train$delta_2)
    beta_1 = beta_1 - step_size * grad_beta(fb_train$delta_2, fb_train$z_1)
    beta_2 = beta_2 - step_size * grad_beta(fb_train$delta_2, fb_train$z_2)
    w_10 = w_10 - step_size * grad_w0(fb_train$delta_11)
    w_20 = w_20 - step_size * grad_w0(fb_train$delta_12)
    w_11 = w_11 - step_size * grad_w1(fb_train$delta_11, x_train)
    w_21 = w_21 - step_size * grad_w1(fb_train$delta_12, x_train)
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    train_p[,i] = fb_train$p
    test_p[,i] = fb_test$p
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_10 = w_10, w_11 = w_11, w_20 = w_20, w_21 = w_21,
                   train_p = train_p, test_p = test_p))
}
```

```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = rnorm(100)
y_train = rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train))
x_test = rnorm(1000)
y_test = rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test))
```

```{r}
set.seed(8)
fit = neural_network_2_units(x_train, y_train, x_test, y_test, step_size = 0.5, iter = 50000) 
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# Continuous Outputs

```{r}
get_t = function(z_1, z_2, alpha, beta_1, beta_2) alpha + beta_1*z_1 + beta_2*z_2
get_s = function(x, w_0, w_1) w_0 + w_1*x
activation = function(s) s
get_error_2 = function(y, p) y - p
get_error_1 = function(delta_2, grad_s, beta) grad_s*beta*delta_2
logistic_activation = function(s) 1/(1 + exp(-s))
grad_logistic = function(s) logistic_activation(s)*(1-logistic_activation(s))
cost_func = function(p, y) mean((y-p)^2)
grad_alpha = function(delta_2) -2*mean(delta_2)
grad_beta = function(delta_2, z) -2*mean(delta_2*z)
grad_w0 = function(delta_1) -2*mean(delta_1)
grad_w1 = function(delta_1, x) -2*mean(delta_1*x)
```

```{r}
forward_backward_pass = function(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21) {
  # compute the linear predictors and hidden units
  s_1 = get_s(x, w_10, w_11)
  z_1 = logistic_activation(s_1)
  s_2 = get_s(x, w_20, w_21)
  z_2 = logistic_activation(s_2)
  # compute the output units
  t_1 = get_t(z_1, z_2, alpha, beta_1, beta_2)
  p = activation(t_1)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s1 = grad_logistic(s_1)
  grad_s2 = grad_logistic(s_2)
  delta_11 = get_error_1(delta_2, grad_s1, beta_1)
  delta_12 = get_error_1(delta_2, grad_s2, beta_2)
  return(ls = list(p=p, delta_2=delta_2, delta_11=delta_11, delta_12=delta_12, 
                   z_1=z_1, z_2=z_2))
}
```

```{r}
neural_network_2_units = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  train_p = matrix(NA, length(x_train), iter)
  test_p = matrix(NA, length(x_test), iter)
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  train_p[,1] = fb_train$p
  test_p[,1] = fb_test$p
  for (i in 2:iter) {
    alpha = alpha - step_size * grad_alpha(fb_train$delta_2)
    beta_1 = beta_1 - step_size * grad_beta(fb_train$delta_2, fb_train$z_1)
    beta_2 = beta_2 - step_size * grad_beta(fb_train$delta_2, fb_train$z_2)
    w_10 = w_10 - step_size * grad_w0(fb_train$delta_11)
    w_20 = w_20 - step_size * grad_w0(fb_train$delta_12)
    w_11 = w_11 - step_size * grad_w1(fb_train$delta_11, x_train)
    w_21 = w_21 - step_size * grad_w1(fb_train$delta_12, x_train)
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    train_p[,i] = fb_train$p
    test_p[,i] = fb_test$p
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_10 = w_10, w_11 = w_11, w_20 = w_20, w_21 = w_21,
                   train_p = train_p, test_p = test_p))
}
```

```{r}
set.seed(8)
x_train = rnorm(100)
y_train = x_train^2 + rnorm(100, sd = 0.5)
x_test = rnorm(1000)
y_test = x_test^2 + rnorm(1000, sd = 0.5)
```

```{r}
set.seed(8)
fit = neural_network_2_units(x_train, y_train, x_test, y_test, step_size = 0.4, iter = 10000)
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# 2 hidden layers

```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = rnorm(100)
y_train = rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train))
x_test = rnorm(1000)
y_test = rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test))
```

```{r}
get_t = function(z_1, z_2, alpha, beta_1, beta_2) alpha + beta_1*z_1 + beta_2*z_2
get_s = function(x, w_0, w_1) w_0 + w_1*x
activation = function(s) 1/(1 + exp(-s))
get_error_2 = function(y, p) y - p
get_error_1 = function(delta_2, grad_s, beta) grad_s*beta*delta_2
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(delta_2) -mean(delta_2)
grad_beta = function(delta_2, z) -mean(delta_2*z)
grad_w0 = function(delta_1) -mean(delta_1)
grad_w1 = function(delta_1, x) -mean(delta_1*x)
```

x -> d(u) -> z(s) -> y

```{r}
forward_backward_pass_2_layer = function(x, y, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21, v_10, v_11, v_20, v_21) {
  # compute the linear predictors and 1st layer hidden units
  u_1 = get_s(x, w_10, w_11)
  d_1 = activation(u_1)
  u_2 = get_s(x, w_20, w_21)
  d_2 = activation(u_2)
  # compute the linear predictors and 2nd layer hidden units
  s_1 = get_s(d_1, v_10, v_11)
  z_1 = activation(s_1)
  s_2 = get_s(d_2, v_20, v_21)
  z_2 = activation(s_2)
  # compute the output units
  t_1 = get_t(z_1, z_2, alpha, beta_1, beta_2)
  p = activation(t_1)
  # compute the errors from the output-hidden layer
  delta_3 = get_error_2(y, p)
  # compute the errors from the 2nd hidden-input layer
  grad_s1 = grad_logistic(s_1)
  grad_s2 = grad_logistic(s_2)
  delta_21 = get_error_1(delta_3, grad_s1, beta_1)
  delta_22 = get_error_1(delta_3, grad_s2, beta_2)
  # compute the errors from the 1st hidden-input layer
  grad_s1 = grad_logistic(u_1)
  grad_s2 = grad_logistic(u_2)
  delta_11 = get_error_1(delta_21, grad_s1, v_11)
  delta_12 = get_error_1(delta_22, grad_s2, v_21)
  return(ls = list(p=p, delta_3=delta_3, delta_11=delta_11, delta_12=delta_12, delta_21=delta_21, delta_22=delta_22, 
                   z_1=z_1, z_2=z_2, d_1 = d_1, d_2 = d_2))
}
```

```{r}
neural_network_2_layer = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  train_p = matrix(NA, length(x_train), iter)
  test_p = matrix(NA, length(x_test), iter)
  alpha = rnorm(1, sd = 0.1)
  beta_1 = rnorm(1, sd = 0.1)
  beta_2 = rnorm(1, sd = 0.1)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_11 = rnorm(1, sd = 0.1)
  w_21 = rnorm(1, sd = 0.1)
  v_10 = rnorm(1, sd = 0.1)
  v_20 = rnorm(1, sd = 0.1)
  v_11 = rnorm(1, sd = 0.1)
  v_21 = rnorm(1, sd = 0.1)
  fb_train = forward_backward_pass_2_layer(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21, v_10, v_11, v_20, v_21)
  fb_test = forward_backward_pass_2_layer(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21, v_10, v_11, v_20, v_21)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  train_p[,1] = fb_train$p
  test_p[,1] = fb_test$p
  for (i in 2:iter) {
    alpha = alpha - step_size * grad_alpha(fb_train$delta_3)
    beta_1 = beta_1 - step_size * grad_beta(fb_train$delta_3, fb_train$z_1)
    beta_2 = beta_2 - step_size * grad_beta(fb_train$delta_3, fb_train$z_2)
    
    v_10 = v_10 - step_size * grad_w0(fb_train$delta_21)
    v_20 = v_20 - step_size * grad_w0(fb_train$delta_22)
    v_11 = v_11 - step_size * grad_w1(fb_train$delta_21, fb_train$d_1)
    v_21 = v_21 - step_size * grad_w1(fb_train$delta_22, fb_train$d_2)
    
    w_10 = w_10 - step_size * grad_w0(fb_train$delta_11)
    w_20 = w_20 - step_size * grad_w0(fb_train$delta_12)
    w_11 = w_11 - step_size * grad_w1(fb_train$delta_11, x_train)
    w_21 = w_21 - step_size * grad_w1(fb_train$delta_12, x_train)

    fb_train = forward_backward_pass_2_layer(x_train, y_train, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21, v_10, v_11, v_20, v_21)
    fb_test = forward_backward_pass_2_layer(x_test, y_test, alpha, beta_1, beta_2, w_10, w_11, w_20, w_21, v_10, v_11, v_20, v_21)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    train_p[,i] = fb_train$p
    test_p[,i] = fb_test$p
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_10 = w_10, w_11 = w_11, w_20 = w_20, w_21 = w_21,
                   v_10 = v_10, v_11 = v_11, v_20 = v_20, v_21 = v_21,
                   train_p = train_p, test_p = test_p))
}
```

```{r}
set.seed(8)
fit = neural_network_2_layer(x_train, y_train, x_test, y_test, step_size = 0.5, iter = 30000) 
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# high-dimensional x

```{r}
get_t = function(z_1, z_2, alpha, beta_1, beta_2) alpha + beta_1*z_1 + beta_2*z_2
get_s = function(x, w_0, w_1) as.vector(w_0 + x %*% w_1)
activation = function(s) 1/(1 + exp(-s))
get_error_2 = function(y, p) y - p
get_error_1 = function(delta_2, grad_s, beta) grad_s*beta*delta_2
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(y*log(p) + (1-y)*log(1-p))
grad_alpha = function(delta_2) -mean(delta_2)
grad_beta = function(delta_2, z) -mean(delta_2*z)
grad_w0 = function(delta_1, x) -mean(delta_1)
grad_w = function(delta_1, x) -mean(delta_1*x)
```

```{r}
forward_backward_pass = function(x, y, alpha, beta_1, beta_2, w_10, w_1, w_20, w_2) {
  # compute the linear predictors and hidden units
  s_1 = get_s(x, w_10, w_1)
  z_1 = activation(s_1)
  s_2 = get_s(x, w_20, w_2)
  z_2 = activation(s_2)
  # compute the output units
  t_1 = get_t(z_1, z_2, alpha, beta_1, beta_2)
  p = activation(t_1)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s1 = grad_logistic(s_1)
  grad_s2 = grad_logistic(s_2)
  delta_11 = get_error_1(delta_2, grad_s1, beta_1)
  delta_12 = get_error_1(delta_2, grad_s2, beta_2)
  return(ls = list(p=p, delta_2=delta_2, delta_11=delta_11, delta_12=delta_12, 
                   z_1=z_1, z_2=z_2))
}
```

```{r}
neural_network_2_units = function(x_train, y_train, x_test, y_test, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  alpha = rnorm(1, sd = 0.01)
  beta_1 = rnorm(1, sd = 0.01)
  beta_2 = rnorm(1, sd = 0.01)
  w_10 = rnorm(1, sd = 0.1)
  w_20 = rnorm(1, sd = 0.1)
  w_1 = rnorm(ncol(x_train), sd = 0.01)
  w_2 = rnorm(ncol(x_train), sd = 0.01)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_1, w_20, w_2)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_1, w_20, w_2)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  message("iter = 1, training cost = ", round(cost_train[1], 3), " test cost = ", round(cost_test[1], 3))
  for (i in 2:iter) {
    alpha = alpha - step_size * (grad_alpha(fb_train$delta_2))
    beta_1 = beta_1 - step_size * (grad_beta(fb_train$delta_2, fb_train$z_1))
    beta_2 = beta_2 - step_size * (grad_beta(fb_train$delta_2, fb_train$z_2))
    w_10 = w_10 - step_size * (grad_w0(fb_train$delta_11, x_train))
    w_20 = w_20 - step_size * (grad_w0(fb_train$delta_12, x_train))
    for (j in 1:ncol(x_train)) {
      w_1[j] = w_1[j] - step_size * (grad_w(fb_train$delta_11, x_train[,j]))
      w_2[j] = w_2[j] - step_size * (grad_w(fb_train$delta_12, x_train[,j]))
    }
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta_1, beta_2, w_10, w_1, w_20, w_2)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta_1, beta_2, w_10, w_1, w_20, w_2)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    if (i %% 50 == 0) message("iter = ", i, " training cost = ", round(cost_train[i], 6), " test cost = ", round(cost_test[i], 6))
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta_1 = beta_1, beta_2 = beta_2, 
                   w_1 = w_1, w_2 = w_2))
}
```

```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = matrix(rnorm(300), 100, 3)
y_train = rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,1]))
x_test = matrix(rnorm(3000), 1000, 3)
y_test = rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,1]))
```

```{r, message = FALSE}
set.seed(8)
fit = neural_network_2_units(x_train, y_train, x_test, y_test, step_size = 3, iter = 10000)
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# multiple z

```{r}
# z is N by D matrix of hidden units, beta is D-vector of weights
get_t = function(z, alpha, beta) as.vector(alpha + z %*% beta)
# w is P by D matrix of weights, w_0 is D-vector of intercepts
# the double transpose thing is for adding w_0 to x %*% w row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_s = function(x, w_0, w) t(t(x %*% w) + w_0)
activation = function(s) 1/(1 + exp(-s))
get_error_2 = function(y, p) y - p
# the double transpose thing is for multiplying grad_s with beta row-wise
# I need that because R do column-wise multiplication when multiplying matrix with vectors
get_error_1 = function(delta_2, grad_s, beta) t(t(grad_s)*beta)*delta_2
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(y*log(p) + (1-y)*log(1-p))
# the gradients for beta and w are simply cross-products of hidden/inputs with the deltas
grad_alpha = function(delta_2) -mean(delta_2)
grad_beta = function(delta_2, z) -as.vector(crossprod(z, delta_2)/nrow(z))
grad_w0 = function(delta_1, x) -colMeans(delta_1)
grad_w = function(delta_1, x) -as.vector(crossprod(x, delta_1)/nrow(x))
```

```{r}
forward_backward_pass = function(x, y, alpha, beta, w_0, w) {
  # compute the linear predictors and hidden units
  s = get_s(x, w_0, w)
  z = activation(s)
  # compute the output units
  t = get_t(z, alpha, beta)
  p = activation(t)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s = grad_logistic(s)
  delta_1 = get_error_1(delta_2, grad_s, beta)
  return(ls = list(p=p, delta_2=delta_2, delta_1=delta_1, z = z))
}
```

```{r}
neural_network = function(x_train, y_train, x_test, y_test, num_hidden = 10, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  alpha = rnorm(1, sd = 0.01)
  beta = rnorm(num_hidden, sd = 0.01)
  w_0 = rnorm(num_hidden, sd = 0.1)
  w = matrix(rnorm(ncol(x_train)*num_hidden, sd = 0.01), ncol(x_train), num_hidden)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta, w_0, w)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta, w_0, w)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  message("iter = 1, training cost = ", round(cost_train[1], 3), " test cost = ", round(cost_test[1], 3))
  for (i in 2:iter) {
    alpha = alpha - step_size * (grad_alpha(fb_train$delta_2))
    beta = beta - step_size * (grad_beta(fb_train$delta_2, fb_train$z))
    w_0 = w_0 - step_size * (grad_w0(fb_train$delta_1, x_train))
    w = w - step_size * (grad_w(fb_train$delta_1, x_train))
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta, w_0, w)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta, w_0, w)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    if (i %% 50 == 0) message("iter = ", i, " training cost = ", round(cost_train[i], 6), " test cost = ", round(cost_test[i], 6))
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta = beta,
                   w_0 = w_0, w_1 = w_1))
}
```

```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = matrix(rnorm(300), 100, 3)
y_train = rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,1]))
x_test = matrix(rnorm(3000), 1000, 3)
y_test = rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,1]))
```

```{r, message = FALSE}
set.seed(8)
fit = neural_network(x_train, y_train, x_test, y_test, num_hidden = 5, step_size = 0.8, iter = 50000)
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# Momentum Gradient Descent

```{r}
neural_network_momentum = function(x_train, y_train, x_test, y_test, num_hidden = 10, iter = 50000, momentum = 0.9, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  alpha = rnorm(1, sd = 0.01)
  beta = rnorm(num_hidden, sd = 0.01)
  w_0 = rnorm(num_hidden, sd = 0.1)
  w = matrix(rnorm(ncol(x_train)*num_hidden, sd = 0.01), ncol(x_train), num_hidden)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta, w_0, w)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta, w_0, w)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  message("iter = 1, training cost = ", round(cost_train[1], 3), " test cost = ", round(cost_test[1], 3))
  
  g_alpha = 0
  g_beta = rep(0, num_hidden)
  g_w_0 = rep(0, num_hidden)
  g_w = matrix(0, ncol(x_train), num_hidden)
  
  for (i in 2:iter) {
    g_alpha = momentum*g_alpha +  (1 - momentum) * grad_alpha(fb_train$delta_2)
    g_beta = momentum*g_beta + (1 - momentum) * grad_beta(fb_train$delta_2, fb_train$z)
    g_w_0 = momentum*g_w_0 + (1 - momentum) * grad_w0(fb_train$delta_1, x_train)
    g_w = momentum*g_w + (1 - momentum) * grad_w(fb_train$delta_1, x_train)
    alpha = alpha - step_size * g_alpha
    beta = beta - step_size * g_beta
    w_0 = w_0 - step_size * g_w_0
    w = w - step_size * g_w
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta, w_0, w)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta, w_0, w)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    if (i %% 50 == 0) message("iter = ", i, " training cost = ", round(cost_train[i], 6), " test cost = ", round(cost_test[i], 6))
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta = beta,
                   w_0 = w_0, w_1 = w_1))
}
```

```{r, message = FALSE}
set.seed(8)
fit = neural_network_momentum(x_train, y_train, x_test, y_test, num_hidden = 5, step_size = 0.8, iter = 10000)
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# Multi-class classification

```{r}
# z is N by D matrix of hidden units, beta is D by K matrix of weights, alpha is K-vector of intercepts
# the double transpose thing is for adding alpha to z %*% beta row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_t = function(z, alpha, beta) t(t(z %*% beta) + alpha)
# w is P by D matrix of weights, w_0 is D-vector of intercepts
# the double transpose thing is for adding w_0 to x %*% w row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_s = function(x, w_0, w) t(t(x %*% w) + w_0)
# soft_max return N by K matrix of output probabilities
soft_max = function(a) exp(a)/rowSums(exp(a))
activation = function(s) 1/(1 + exp(-s))
# 2nd-layer-error by a N-K matrix of errors
get_error_2 = function(y, p) y - p
# the double transpose thing is for multiplying grad_s with beta row-wise
# I need that because R do column-wise multiplication when multiplying matrix with vectors
get_error_1 = function(delta_2, grad_s, beta) grad_s * tcrossprod(delta_2, beta)
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(rowSums(y*log(p)))
# the gradients for beta and w are simply cross-products of hidden/inputs with the deltas
grad_alpha = function(delta_2) -colMeans(delta_2)
grad_beta = function(delta_2, z) -crossprod(z, delta_2)/nrow(z)
grad_w0 = function(delta_1, x) -colMeans(delta_1)
grad_w = function(delta_1, x) -crossprod(x, delta_1)/nrow(x)
```

```{r}
forward_backward_pass = function(x, y, alpha, beta, w_0, w) {
  # compute the linear predictors and hidden units
  s = get_s(x, w_0, w)
  z = activation(s)
  # compute the output units
  t = get_t(z, alpha, beta)
  p = soft_max(t)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s = grad_logistic(s)
  delta_1 = get_error_1(delta_2, grad_s, beta)
  return(ls = list(p=p, delta_2=delta_2, delta_1=delta_1, z = z))
}
```

```{r}
neural_network = function(x_train, y_train, x_test, y_test, num_hidden = 10, iter = 10000, step_size = 0.5) {
  cost_train = cost_test = rep(NA, iter)
  alpha = rnorm(ncol(y_test), sd = 0.01)
  beta = matrix(rnorm(num_hidden*ncol(y_test), sd = 0.01), num_hidden, ncol(y_test))
  w_0 = rnorm(num_hidden, sd = 0.1)
  w = matrix(rnorm(ncol(x_train)*num_hidden, sd = 0.01), ncol(x_train), num_hidden)
  fb_train = forward_backward_pass(x_train, y_train, alpha, beta, w_0, w)
  fb_test = forward_backward_pass(x_test, y_test, alpha, beta, w_0, w)
  cost_train[1] = cost_func(fb_train$p, y_train)
  cost_test[1] = cost_func(fb_test$p, y_test)
  message("iter = 1, training cost = ", round(cost_train[1], 3), " test cost = ", round(cost_test[1], 3))
  for (i in 2:iter) {
    alpha = alpha - step_size * (grad_alpha(fb_train$delta_2))
    beta = beta - step_size * (grad_beta(fb_train$delta_2, fb_train$z))
    w_0 = w_0 - step_size * (grad_w0(fb_train$delta_1, x_train))
    w = w - step_size * (grad_w(fb_train$delta_1, x_train))
    fb_train = forward_backward_pass(x_train, y_train, alpha, beta, w_0, w)
    fb_test = forward_backward_pass(x_test, y_test, alpha, beta, w_0, w)
    cost_train[i] = cost_func(fb_train$p, y_train)
    cost_test[i] = cost_func(fb_test$p, y_test)
    if (i %% 50 == 0) message("iter = ", i, " training cost = ", round(cost_train[i], 6), " test cost = ", round(cost_test[i], 6))
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   alpha = alpha, beta = beta,
                   w_0 = w_0, w = w))
}
```

```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = matrix(rnorm(300), 100, 3)
y_train = factor(rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,1])) + 
  rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,2])))
x_test = matrix(rnorm(3000), 1000, 3)
y_test = factor(rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,1])) +
  rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,2])))
y_train = model.matrix(~ y_train - 1)
y_test = model.matrix(~ y_test - 1)
```

```{r, message = FALSE}
set.seed(8)
fit = neural_network(x_train, y_train, x_test, y_test, num_hidden = 5, step_size = 1.5, iter = 20000)
max(diff(fit$cost_train))
```

```{r}
system.time(neural_network(x_train, y_train, x_test, y_test, num_hidden = 5, step_size = 1.5, iter = 20000))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

# Multi-class classification: enhanced code

```{r}
# z is N by D matrix of hidden units, beta is D by K matrix of weights, alpha is K-vector of intercepts
# the double transpose thing is for adding alpha to z %*% beta row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_t = function(z, beta) z %*% beta
# w is P by D matrix of weights, w_0 is D-vector of intercepts
# the double transpose thing is for adding w_0 to x %*% w row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_s = function(x, w) x %*% w
# soft_max return N by K matrix of output probabilities
soft_max = function(a) exp(a)/rowSums(exp(a))
activation = function(s) 1/(1 + exp(-s))
# 2nd-layer-error by a N-K matrix of errors
get_error_2 = function(y, p) y - p
# the double transpose thing is for multiplying grad_s with beta row-wise
# I need that because R do column-wise multiplication when multiplying matrix with vectors
get_error_1 = function(delta_2, grad_s, beta) grad_s * tcrossprod(delta_2, beta[-1,])
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(rowSums(y*log(p)))
# the gradients for beta and w are simply cross-products of hidden/inputs with the deltas
grad_beta = function(delta_2, z) -crossprod(z, delta_2)/nrow(z)
grad_w = function(delta_1, x) -crossprod(x, delta_1)/nrow(x)
```

```{r}
forward_backward_pass = function(x, y, beta, w) {
  # compute the linear predictors and hidden units
  s = get_s(x, w)
  z = cbind(rep(1, nrow(x)), activation(s))
  # compute the output units
  t = get_t(z, beta)
  p = soft_max(t)
  # compute the errors from the output-hidden layer
  delta_2 = get_error_2(y, p)
  # compute the errors from the hidden-input layer
  grad_s = grad_logistic(s)
  delta_1 = get_error_1(delta_2, grad_s, beta)
  return(ls = list(p=p, delta_2=delta_2, delta_1=delta_1, z = z))
}
```

```{r}
neural_network = function(x_train, y_train, x_test, y_test, num_hidden = 10, iter = 10000, step_size = 0.5, lambda = 1e-5, momentum = 0.9) {
  
  x_train = cbind(rep(1, nrow(x_train)), x_train)
  x_test = cbind(rep(1, nrow(x_test)), x_test)
  cost_train = cost_test = rep(NA, iter)
  alpha = rnorm(ncol(y_test), sd = 0.01)
  beta = matrix(rnorm((num_hidden + 1)*ncol(y_test), sd = 0.01), num_hidden + 1, ncol(y_test))
  w = matrix(rnorm(ncol(x_train)*num_hidden, sd = 0.01), ncol(x_train), num_hidden)
  fb_train = forward_backward_pass(x_train, y_train, beta, w)
  fb_test = forward_backward_pass(x_test, y_test, beta, w)
  penalty = lambda/2*(sum(beta[-1,])^2 + sum(w[-1,])^2)
  cost_train[1] = cost_func(fb_train$p, y_train) + penalty
  cost_test[1] = cost_func(fb_test$p, y_test)
  message("iter = 1, training cost = ", round(cost_train[1], 3), " test cost = ", round(cost_test[1], 3))

  g_beta = rep(0, num_hidden + 1)
  g_w = matrix(0, ncol(x_train), num_hidden)
  
  for (i in 2:iter) {
    pen_beta = lambda * beta
    pen_w = lambda * w
    pen_beta[1, ] = 0
    pen_w[1, ] = 0
    
    g_beta = momentum*g_beta + (1 - momentum) * (grad_beta(fb_train$delta_2, fb_train$z) + pen_beta)
    g_w = momentum*g_w + (1 - momentum) * (grad_w(fb_train$delta_1, x_train) + pen_w)
    
    beta = beta - step_size * g_beta
    w = w - step_size * g_w
    fb_train = forward_backward_pass(x_train, y_train, beta, w)
    fb_test = forward_backward_pass(x_test, y_test, beta, w)
    penalty = lambda/2*(sum(beta[-1,])^2 + sum(w[-1,])^2)
    cost_train[i] = cost_func(fb_train$p, y_train) + penalty
    cost_test[i] = cost_func(fb_test$p, y_test)
    if (i %% 50 == 0) message("iter = ", i, " training cost = ", round(cost_train[i], 6), " test cost = ", round(cost_test[i], 6))
  }
  return(ls = list(cost_train = cost_train, cost_test = cost_test,
                   beta = beta, w = w))
}
```

```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = matrix(rnorm(300), 100, 3)
y_train = factor(rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,1])) + 
  rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,2])))
x_test = matrix(rnorm(3000), 1000, 3)
y_test = factor(rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,1])) +
  rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,2])))
y_train = model.matrix(~ y_train - 1)
y_test = model.matrix(~ y_test - 1)
```

```{r, message = FALSE}
set.seed(8)
fit = neural_network(x_train, y_train, x_test, y_test, num_hidden = 5, step_size = 5, iter = 20000)
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```

```{r}
system.time(neural_network(x_train, y_train, x_test, y_test, num_hidden = 5, step_size = 1.5, iter = 20000))
```

# Multi-class mulit-layer classification: enhanced code

```{r}
# z is N by D matrix of hidden units, beta is D by K matrix of weights, alpha is K-vector of intercepts
# the double transpose thing is for adding alpha to z %*% beta row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_t = function(z, beta) z %*% beta
# w is P by D matrix of weights, w_0 is D-vector of intercepts
# the double transpose thing is for adding w_0 to x %*% w row-wise
# I need that because R do column-wise addition when adding matrix with vectors
get_s = function(x, w) x %*% w
# soft_max return N by K matrix of output probabilities
soft_max = function(a) exp(a)/rowSums(exp(a))
activation = function(s) 1/(1 + exp(-s))
# 2nd-layer-error by a N-K matrix of errors
get_error_2 = function(y, p) y - p
# the double transpose thing is for multiplying grad_s with beta row-wise
# I need that because R do column-wise multiplication when multiplying matrix with vectors
get_error_1 = function(delta_2, grad_s, beta) {
  if (nrow(beta) > 2) grad_s * tcrossprod(delta_2, beta[-1,])
  else grad_s * (delta_2 %*% beta[-1,])
}
grad_logistic = function(s) activation(s)*(1-activation(s))
cost_func = function(p, y) -mean(rowSums(y*log(p)))
# the gradients for beta and w are simply cross-products of hidden/inputs with the deltas
grad_beta = function(delta_2, z) -crossprod(z, delta_2)/nrow(z)
grad_w = function(delta_1, x) -crossprod(x, delta_1)/nrow(x)
```

```{r}
forward_backward_pass = function(x, y, w) {
  
  # w is a list of weights, from inputs to outputs through the layers
  # length of w is number of hidden units + 1 (for the output layer)
  # z_list stores the inputs, 1st layer hidden, 2nd layer hidden, etc.
  # s_list stores the corresponding linear predictors
  
  s_list = vector("list", length(w) - 1)
  z_list = vector("list", length(w))
  
  z_list[[1]] = x
  
  # compute the linear predictors and hidden units over the layers
  
  for (i in 2:length(z_list)) {
    s_list[[i-1]] = get_s(z_list[[i-1]], w[[i-1]])
    z_list[[i]] = cbind(rep(1, nrow(x)), activation(s_list[[i-1]]))
  }
  
  # compute the output units
  
  t = get_t(z_list[[length(z_list)]], w[[length(w)]])
  p = soft_max(t)
  
  # delta_list stores the delta from 
  # output -> last hidden layer -> 2nd last hidden layer etc.
  
  delta_list = vector("list", length(w))
  
  # compute the errors from the output-hidden layer
  
  delta_list[[length(w)]] = get_error_2(y, p)

  # compute the errors from the hidden-input layer propagating backwards
  
  for (i in (length(w) - 1):1) {
    grad_s = grad_logistic(s_list[[i]])
    delta_list[[i]] = get_error_1(delta_list[[i+1]], grad_s, w[[i+1]])
  }
  
  return(ls = list(p=p, delta = delta_list, z = z_list))

}
```

```{r}
neural_network = function(x_train, y_train, x_test, y_test, num_hidden = c(3, 3), 
                          iter = 10000, step_size = 0.5, lambda = 1e-5, momentum = 0.9,
                          ini_w = NULL, sparse = FALSE) {
  
  num_p = ncol(x_train)
  x_train = cbind(rep(1, nrow(x_train)), x_train)
  x_test = cbind(rep(1, nrow(x_test)), x_test)
  y_train = model.matrix(~ y_train - 1)
  y_test = model.matrix(~ y_test - 1)
  cost_train = cost_test = rep(NA, iter)
  
  if (sparse) {
    require(Matrix)
    x_train = Matrix(x_train)
    x_test = Matrix(x_test)
    y_train = Matrix(y_train)
    y_test = Matrix(y_test)
  }

  if (is.null(ini_w)) {
    num_hidden = c(num_p, num_hidden, ncol(y_test))
    w = vector("list", length(num_hidden) - 1)
    for (i in 1:(length(num_hidden) - 1)) {
      w[[i]] = matrix(rnorm((num_hidden[i] + 1)*num_hidden[i+1], sd = 0.1), num_hidden[i] + 1, num_hidden[i+1])
    }
  }
  
  # if (is.null(ini_beta)) beta = matrix(rnorm((num_hidden + 1)*ncol(y_test), sd = 0.01), num_hidden + 1, ncol(y_test))
  # else beta = ini_beta
  # if (is.null(ini_w)) w = matrix(rnorm(ncol(x_train)*num_hidden, sd = 0.01), ncol(x_train), num_hidden)
  # else w = ini_w
  
  fb_train = forward_backward_pass(x_train, y_train, w)
  penalty = lambda/2*sum(sapply(w, function(x) sum(x[-1,]^2)))
  cost_train[1] = cost_func(fb_train$p, y_train) + penalty
  
  fb_test = forward_backward_pass(x_test, y_test, w)
  cost_test[1] = cost_func(fb_test$p, y_test)
  message("iter = 1, training cost = ", round(cost_train[1], 6), " test cost = ", round(cost_test[1], 6))

  g_w = vector("list", length(w))
  
  for (j in 1:length(w)) {
    g_w[[j]] = matrix(0, num_hidden[j] + 1, num_hidden[j+1])
  }
  
  for (i in 2:iter) {
    
    for (j in 1:length(w)) {
      pen_w = lambda * w[[j]]
      pen_w[1, ] = 0
      
      g_w[[j]] = momentum*g_w[[j]] + (1 - momentum) * (grad_w(fb_train$delta[[j]], fb_train$z[[j]]) + pen_w)
      
      w[[j]] = w[[j]] - step_size * g_w[[j]]
    }
    
    fb_train = forward_backward_pass(x_train, y_train, w)
    penalty = lambda/2*sum(sapply(w, function(x) sum(x[-1,]^2)))
    cost_train[i] = cost_func(fb_train$p, y_train) + penalty
    
    fb_test = forward_backward_pass(x_test, y_test, w)
    cost_test[i] = cost_func(fb_test$p, y_test)
    message("iter = ", i, " training cost = ", round(cost_train[i], 6), " test cost = ", round(cost_test[i], 6))
    
    
  }
  
  return(ls = list(cost_train = cost_train, cost_test = cost_test, w = w))
  
}
```


```{r}
logistic = function(alpha, beta, x) 1/(1 + exp(-(alpha + beta*x)))
```

```{r}
set.seed(8)
x_train = matrix(rnorm(300), 100, 3)
y_train = factor(rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,1])) + 
  rbinom(100, 1, prob = logistic(alpha = 0, beta = 1, x_train[,2])))
x_test = matrix(rnorm(3000), 1000, 3)
y_test = factor(rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,1])) +
  rbinom(1000, 1, prob = logistic(alpha = 0, beta = 1, x_test[,2])))
y_train = model.matrix(~ y_train - 1)
y_test = model.matrix(~ y_test - 1)
```

```{r, message = FALSE}
fit = neural_network(x_train, y_train, x_test, y_test, step_size = 0.5, num_hidden = c(3,1,3), iter = 50000)
max(diff(fit$cost_train))
```

```{r, fig.width = 4, fig.height = 2.5}
data.frame(training_cost = fit$cost_train, test_cost = fit$cost_test) %>%
  mutate(iter = 1:length(fit$cost_train)) %>%
  pivot_longer(!iter, names_to = "set", values_to = "cost") %>%
  ggplot(aes(x = iter, y = cost, col = set)) +
  geom_line(lwd = 1) +
  scale_y_log10() +
  xlab("iteration")
```